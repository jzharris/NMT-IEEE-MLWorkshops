{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras and the dataset and Network models\n",
    "\n",
    "Keras is a top-level library we will use to interface with TensorFlow. Keras will take care of many things that we usually have to do by hand in TF, and it is easier to understand and use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-67c38c291a34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import TensorFlow and tools to save the network weights\n",
    "\n",
    "We still need to import TensorFlow. We will be using two built-in functions in TensorFlow to \"freeze\", or save, our \"graph\". These are the trained weights that our algorithm will produce. We will save them so we only have to save once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ba70168cc7d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfreeze_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimize_for_inference_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data function\n",
    "\n",
    "This function takes the common mnist dataset and grabs a \"training\" and \"test\" dataset from it. Each dataset consists of x = 28x28 pixel images and y = the numbers they represent.\n",
    "\n",
    "The pixel values are converted from standard pixel values (0 to 255) to fractions (0 to 1) for easier computations. The number values are put into a \"one-hot encoded\" array.\n",
    "\n",
    "Ex: A \"one-hot encoded\" array for the number \"1\" will look like this: [0 1 0 0 0 0 0 0 0 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model function\n",
    "\n",
    "This is our network! We \"stack\" layers on top of one another just like lasagna. We enable this in Keras by creating a \"Sequential\" type model. To add a layer, we call the .add method. This method takes in one of the layer types we imported. We will use two main layers: Convolutional and MaxPooling2D.\n",
    "\n",
    "Convolutional layers are layers that train a filter to look at the input image.\n",
    "\n",
    "MaxPooling2D layers simply take the maximum output of a certain number of pixels and remove the rest. This is a good way of \"abstracting\" the image, and simplifying the output for the next Convolutional Layer to analyze. The output would be similar if you look at the original image through a foggy lense.\n",
    "\n",
    "Layer 1: Convolutional layer - parameters are standard for image filters\n",
    "\t\n",
    "\tfilters:\t64 \t(neurons in the layer)\n",
    "\tkernel_size:\t3\t(how many pixels to look at)\n",
    "\tstrides:\t1\t(how many pixels to move once done analyzing)\n",
    "\tpadding:\t'same'\t(automatically pad)\n",
    "\tactivation:\t'relu'\t(negative values set to 0)\n",
    "\tinput_shape:\t28x28x1\t(analyzes 1 28x28 pixel image at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=64, kernel_size=3, strides=1, \\\n",
    "            padding='same', activation='relu', \\\n",
    "            input_shape=[28, 28, 1]))\n",
    "    # 28*28*64\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "    # 14*14*64\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=3, strides=1, \\\n",
    "            padding='same', activation='relu'))\n",
    "    # 14*14*128\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "    # 7*7*128\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=3, strides=1, \\\n",
    "            padding='same', activation='relu'))\n",
    "    # 7*7*256\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "    # 4*4*256\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function\n",
    "\n",
    "Keras function .compile takes a model that we passed it (the ie the network we built up above) and compiles it. This gets it ready to train.\n",
    "\tloss:\t\tcategorical_crossentropy\t(loss function for our network)\n",
    "\toptimizer:\tadadelta\t\t\t(how we will adjust our weights)\n",
    "\tmetrics:\t'accuracy'\t\t\t(what we want to measure progress by)\n",
    "\n",
    "Then we call .fit. This runs our model, with the specified batch size, epochs, and training/test data. It calculates the accuracy after each batch, updates the weights, runs the next batch, and then repeats this process for the number of epochs specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, x_test, y_test):\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \\\n",
    "                  optimizer=keras.optimizers.Adadelta(), \\\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train, \\\n",
    "              batch_size=BATCH_SIZE, \\\n",
    "              epochs=EPOCHS, \\\n",
    "              verbose=1, \\\n",
    "              validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export function\n",
    "\n",
    "This function saves the weights of our model once it has been trained into a file that TensorFlow can reuse in another program. The catch is that the network layers and parameters in those layers must be the same, otherwise TensorFlow wouldn't know which weights go where."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(saver, model, input_node_names, output_node_name):\n",
    "    tf.train.write_graph(K.get_session().graph_def, 'out', \\\n",
    "        MODEL_NAME + '_graph.pbtxt')\n",
    "\n",
    "    saver.save(K.get_session(), 'out/' + MODEL_NAME + '.chkp')\n",
    "\n",
    "    freeze_graph.freeze_graph('out/' + MODEL_NAME + '_graph.pbtxt', None, \\\n",
    "        False, 'out/' + MODEL_NAME + '.chkp', output_node_name, \\\n",
    "        \"save/restore_all\", \"save/Const:0\", \\\n",
    "        'out/frozen_' + MODEL_NAME + '.pb', True, \"\")\n",
    "\n",
    "    input_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.Open('out/frozen_' + MODEL_NAME + '.pb', \"rb\") as f:\n",
    "        input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "            input_graph_def, input_node_names, [output_node_name],\n",
    "            tf.float32.as_datatype_enum)\n",
    "\n",
    "    with tf.gfile.FastGFile('out/opt_' + MODEL_NAME + '.pb', \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "    print(\"graph saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running our program\n",
    "\n",
    "Now, let's run our program using these functions!\n",
    "\tOur model name will be 'mnist_convnet'\n",
    "\tOur epochs will be 1 (usually this number is between 10 and 50)\n",
    "\tOur batch-size will be 128 (takes 128 images at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7e8f76626a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create 'out' directory if not already created\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# load mnist dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "MODEL_NAME = 'mnist_convnet'\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "import os\n",
    "import os.path as path\n",
    "\n",
    "# create 'out' directory if not already created\n",
    "if not path.exists('out'):\n",
    "    os.mkdir('out')\n",
    "\n",
    "# load mnist dataset\n",
    "x_train, y_train, x_test, y_test = load_data()\n",
    "\n",
    "# build model\n",
    "model = build_model()\n",
    "\n",
    "# train model\n",
    "train(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# export model and save weights\n",
    "export_model(tf.train.Saver(), model, [\"conv2d_1_input\"], \"dense_2/Softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}